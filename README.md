<div align="center">
  <img src="banner.svg" alt="Invictus Banner" width="900"/>
</div>

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.7%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.5%2B-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.8.9%2B-0097A7?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Experimental-orange?style=for-the-badge)]()

</div>

---

> ‚ö†Ô∏è **Disclaimer:** This is a research tool only. Lie detection based on facial analysis is not scientifically validated and must **not** be used in legal, professional, or high-stakes contexts. Results are probabilistic and experimental.

---

## üìã Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Requirements](#-requirements)
- [Installation](#-installation)
- [Usage Guide](#-usage-guide)
- [Configuration Reference](#-configuration-reference)
- [How It Works](#-how-it-works)
- [Use Cases](#-use-cases)
- [Known Limitations](#-known-limitations)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## üîç Overview

**Invictus** is a desktop application that uses real-time facial analysis, computer vision, and machine learning to estimate deception probability. It captures live webcam video, extracts facial biometric metrics frame-by-frame, compares them against a calibrated baseline, and outputs a probabilistic deception score.

The system is built entirely in Python using industry-standard libraries and provides a full graphical user interface (GUI) with live graphs, metrics, and exportable results.

---

## ‚ú® Features

| Category | Feature |
|---|---|
| üé• **Video** | Real-time webcam feed with frame-by-frame processing |
| üëÅÔ∏è **Eye Tracking** | Eye Aspect Ratio (EAR) calculation for blink analysis |
| üí° **Blink Detection** | Blink rate tracking (blinks per minute) with edge-triggered counting |
| üé≠ **Emotion** | Dominant emotion detection using FER (Facial Emotion Recognition) |
| üìê **Asymmetry** | Facial symmetry analysis via landmark geometry |
| ‚ö° **Microexpressions** | Rapid landmark velocity change detection |
| ü§ñ **ML Model** | Random Forest Classifier for deception probability scoring |
| üìä **Graphs** | Live and post-analysis matplotlib charts embedded in GUI |
| ‚öôÔ∏è **Calibration** | 10-second baseline capture with outlier filtering |
| üíæ **Export** | Save results as CSV + text report |
| üñ•Ô∏è **GUI** | Full Tkinter desktop interface (1280√ó720, resizable) |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      LieDetectorApp (GUI)                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Video Feed  ‚îÇ    ‚îÇ   Controls   ‚îÇ    ‚îÇ  Right Panel    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (640√ó480)  ‚îÇ    ‚îÇ  + Settings  ‚îÇ    ‚îÇ  Metrics+Graphs ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                     ‚îÇ            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ               Background Video Thread                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         (daemon, thread-safe via shared Lock)            ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ       EnhancedLieDetector         ‚îÇ
          ‚îÇ                                   ‚îÇ
          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
          ‚îÇ  ‚îÇ    MediaPipe FaceMesh        ‚îÇ  ‚îÇ
          ‚îÇ  ‚îÇ    (468 landmark points)     ‚îÇ  ‚îÇ
          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
          ‚îÇ             ‚îÇ                     ‚îÇ
          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
          ‚îÇ  ‚îÇ     Metric Extraction        ‚îÇ  ‚îÇ
          ‚îÇ  ‚îÇ  EAR ¬∑ Asymmetry ¬∑ Blink     ‚îÇ  ‚îÇ
          ‚îÇ  ‚îÇ  Microexpression ¬∑ Emotion   ‚îÇ  ‚îÇ
          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
          ‚îÇ             ‚îÇ                     ‚îÇ
          ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
          ‚îÇ  ‚îÇ   Random Forest Classifier   ‚îÇ  ‚îÇ
          ‚îÇ  ‚îÇ   Deception Probability %    ‚îÇ  ‚îÇ
          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
Webcam ‚Üí OpenCV ‚Üí Resize (0.5√ó) ‚Üí MediaPipe ‚Üí 468 Landmarks
  ‚Üí EAR, Asymmetry, Blink Rate, Eye Movement
  ‚Üí FER Emotion Detection
  ‚Üí Microexpression Velocity Analysis
  ‚Üí Baseline Deviation Scoring
  ‚Üí RandomForest Predict ‚Üí Deception % ‚Üí GUI + Graphs
```

---

## üì¶ Requirements

### Software

| Package | Version | Purpose |
|---|---|---|
| Python | 3.7+ | Runtime |
| `opencv-python` | ‚â• 4.5.0 | Video capture & frame processing |
| `mediapipe` | ‚â• 0.8.9 | 468-point facial landmark detection |
| `numpy` | ‚â• 1.19.0 | Numerical computations |
| `pillow` | ‚â• 8.0.0 | Image conversion for Tkinter display |
| `pandas` | ‚â• 1.3.0 | Metrics storage and DataFrame analysis |
| `matplotlib` | ‚â• 3.4.0 | Embedded real-time graphs |
| `scikit-learn` | ‚â• 0.24.0 | Random Forest Classifier |
| `fer` | ‚â• 22.4.0 | Facial Emotion Recognition |
| `tkinter` | built-in | Desktop GUI framework |

### Hardware

| Component | Minimum | Recommended |
|---|---|---|
| CPU | Dual-core 2GHz | Quad-core 3GHz+ |
| RAM | 4 GB | 8 GB+ |
| Webcam | 640√ó480 capture @ 15fps | 1280√ó720 capture @ 30fps |
| Display | 1280√ó720 | 1920√ó1080 |
| Storage | 200 MB free | 500 MB free |

---

## üöÄ Installation

### 1. Clone the repository

```bash
git clone https://github.com/Kaelith69/invictus.git
cd invictus
```

### 2. Create a virtual environment (recommended)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

> **Note:** `tkinter` is included with Python's standard library. If it's missing on Linux, install it with:
> ```bash
> sudo apt-get install python3-tk
> ```

### 4. Verify the facial landmark model

The file `shape_predictor_68_face_landmarks.dat` should be present in the project root (used by optional dlib integration). The primary detection pipeline uses MediaPipe and does not require this file.

### 5. Run the application

```bash
python invictus.py
```

---

## üìñ Usage Guide

### Step 1 ‚Äî Launch & Disclaimer

Run `python invictus.py`. Accept the disclaimer popup confirming you understand the experimental nature of the tool.

### Step 2 ‚Äî Start Camera

Click **"Start Camera"**. The webcam feed will appear in the left panel. Ensure:
- Your face is clearly visible and well-lit
- You are 40‚Äì80 cm from the camera
- No strong backlighting behind you

### Step 3 ‚Äî Capture Baseline

Click **"Capture Baseline"** and maintain a **neutral resting expression** for **10 seconds**. The progress bar tracks completion. The baseline establishes your personal normal metrics for:
- Eye Aspect Ratio (EAR)
- Facial asymmetry
- Blink rate

> ‚ö†Ô∏è Baseline quality is critical. Poor lighting or expressions during baseline will skew all subsequent analysis.

### Step 4 ‚Äî Configure Settings

In the **Settings** panel:

| Setting | Description | Default |
|---|---|---|
| Recording Duration | How long the analysis session lasts (5‚Äì300s) | 30 seconds |
| Sensitivity | Scales the deception probability output (0.1‚Äì1.0) | 0.7 |
| Question | The question to display during analysis | (empty) |

### Step 5 ‚Äî Start Analysis

Click **"Start Analysis"**. Real-time metrics appear on the right panel and graphs update live:
- **Top graph:** Deception Probability % over time
- **Bottom graph:** Normalized EAR, facial asymmetry, and blink rate over time

### Step 6 ‚Äî Review Results

After analysis completes (or after clicking **"Stop"**), the Results panel shows:
- Average deception probability
- Peak deception probability
- Average blink rate
- Average EAR and facial asymmetry
- Most common emotion
- Number of microexpression events

### Step 7 ‚Äî Save Results

Click **"Save Results"** to export:
- **`results.csv`** ‚Äî full per-frame metrics data
- **`results_report.txt`** ‚Äî human-readable summary

---

## ‚öôÔ∏è Configuration Reference

### Constants (top of `invictus.py`)

| Constant | Default | Description |
|---|---|---|
| `BASELINE_DURATION` | `10` | Baseline capture duration in seconds |
| `MAX_FRAMES` | `10000` | Maximum frames stored in the rolling deque |
| `FRAME_SCALE` | `0.5` | Frame downscale factor for performance |
| `BLINK_MIN_DURATION` | `0.1` | Minimum seconds between registered blinks |
| `VIDEO_WIDTH` | `640` | Display panel width in pixels (frames are resized to this for display) |
| `VIDEO_HEIGHT` | `480` | Display panel height in pixels (frames are resized to this for display) |

### Emotion Weights (in `EnhancedLieDetector`)

These weights modify the deception score based on detected emotion:

| Emotion | Weight | Rationale |
|---|---|---|
| `fear` | +0.4 | Strong deception indicator |
| `surprise` | +0.3 | Moderate indicator |
| `angry` | +0.2 | Moderate indicator |
| `disgust` | +0.2 | Moderate indicator |
| `sad` | +0.1 | Weak indicator |
| `neutral` | 0.0 | No modification |
| `happy` | -0.1 | Slight negative modifier |

---

## üî¨ How It Works

### Eye Aspect Ratio (EAR)

EAR measures eye openness using 6 landmark points per eye:

```
         p2    p3
          \   /
    p1 ----+---- p4
          /   \
         p6    p5

EAR = (||p2-p6|| + ||p3-p5||) / (2 √ó ||p1-p4||)
```

A low EAR (< threshold) indicates a blink. The threshold is dynamically set to 70% of the baseline EAR.

### Blink Rate

Blinks are counted using **edge-triggered detection**: a blink registers exactly once when the EAR first drops below threshold for 2+ consecutive frames (not repeatedly while closed). Rate = blinks in the last 60 seconds.

### Facial Asymmetry

Asymmetry is computed as the relative difference between left and right distances from the nose tip to each cheek landmark:

```
asymmetry = |dist_left - dist_right| / max(dist_left, dist_right)
```

### Microexpression Detection

Landmark velocities between consecutive frames are computed as the sum of displacements across all 468 points. A microexpression is flagged when any frame's velocity exceeds `2 √ó sensitivity √ó mean_velocity`.

### Deception Probability

The Random Forest model receives 5 features:

1. **EAR deviation** ‚Äî how much current EAR deviates from baseline
2. **Asymmetry score** ‚Äî asymmetry relative to baseline
3. **Blink deviation** ‚Äî blink rate deviation from baseline
4. **Eye movement score** ‚Äî mean displacement of eye centers
5. **Emotion score** ‚Äî weighted emotion signal

The output probability is scaled by the `sensitivity` setting and clamped to [0, 100]%.

> **Note:** The included model is trained on synthetic data. Production use would require a validated real-world dataset.

---

## üí° Use Cases

| Use Case | Notes |
|---|---|
| **Research** | Studying correlations between facial signals and truthfulness |
| **Education** | Teaching computer vision and biometric analysis concepts |
| **Prototype Development** | Base for building more sophisticated multimodal systems |
| **Demonstration** | Showcasing real-time ML inference in a desktop application |

---

## ‚ö†Ô∏è Known Limitations

### Technical

- Requires consistent, even lighting (avoid strong shadows or backlighting)
- Face must be clearly visible; glasses, masks, or heavy facial hair reduce accuracy
- Processing delay may occur on systems with CPUs below 2GHz
- Emotion detection via FER may be unreliable on low-resolution or poorly lit faces

### Scientific

- Not validated for legal, professional, or any high-stakes use
- Results are probabilistic and not diagnostic
- Baseline must be captured in the same session and lighting conditions
- Individual physiological differences significantly affect results
- The ML model uses synthetic training data (placeholder)

---

## üîß Troubleshooting

| Problem | Likely Cause | Solution |
|---|---|---|
| "Could not open video source" | Webcam not detected or in use | Close other apps using the webcam; check device index (`video_source = 1`) |
| "No face detected" overlay | Face out of frame or poor lighting | Move closer to camera; improve lighting |
| Baseline failed ("not enough data") | No face visible during baseline | Ensure face is clearly detected before clicking Capture Baseline |
| `ImportError: No module named 'fer'` | Missing dependency | Run `pip install fer` |
| `ImportError: No module named 'mediapipe'` | Missing dependency | Run `pip install mediapipe` |
| Tkinter not found (Linux) | Missing system package | Run `sudo apt-get install python3-tk` |
| Very high/low deception scores always | Poor baseline quality | Recapture baseline in neutral, well-lit conditions |
| App freezes on close | Thread shutdown timing | Use the Stop button before closing the window |

---

## üìÅ Project Structure

```
invictus/
‚îú‚îÄ‚îÄ invictus.py                          # Main application (GUI + detection engine)
‚îú‚îÄ‚îÄ requirements.txt                     # Python dependencies
‚îú‚îÄ‚îÄ banner.svg                           # Project banner image
‚îú‚îÄ‚îÄ README.md                            # This documentation
‚îú‚îÄ‚îÄ shape_predictor_68_face_landmarks.dat # Pre-trained dlib model (optional)
‚îî‚îÄ‚îÄ .gitignore                           # Git ignore rules
```

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/your-feature`
3. **Make** your changes with clear, minimal commits
4. **Test** your changes locally
5. **Open** a Pull Request with a clear description

### Areas for Improvement

- Replace synthetic ML training data with a validated real-world dataset
- Add LSTM/temporal modeling for sequence-based prediction
- Implement multi-face tracking support
- Add audio analysis (voice stress) as a parallel channel
- Improve microexpression detection with optical flow

---

## üìÑ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [**MediaPipe**](https://mediapipe.dev/) ‚Äî Real-time face mesh (468 landmarks)
- [**FER**](https://github.com/justinshenk/fer) ‚Äî Facial Emotion Recognition library
- [**OpenCV**](https://opencv.org/) ‚Äî Computer vision and video capture
- [**Scikit-learn**](https://scikit-learn.org/) ‚Äî Machine learning framework
- [**dlib**](http://dlib.net/) ‚Äî Shape predictor model

---

<div align="center">
  <sub>Built with ‚ù§Ô∏è for research. Use responsibly.</sub>
</div>
